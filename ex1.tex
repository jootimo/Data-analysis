  \documentclass[a4paper]{article}

  %% Language and font encodings
  \usepackage[english]{babel}
  \usepackage[utf8x]{inputenc}
  \usepackage[T1]{fontenc}

  %% Sets page size and margins
  \usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

  %% Useful packages
  \usepackage{amsmath}
  \usepackage{graphicx}
  \usepackage[colorinlistoftodos]{todonotes}
  \usepackage[colorlinks=true, allcolors=blue]{hyperref}
  \usepackage{algorithm2e}
  \usepackage{hyperref}
  \usepackage{listings}


  \title{Applications of Data Analysis\\Excercise 1}
  \author{Timo Heinonen}

  \begin{document}
  \maketitle

  \section{Introduction}
  I chose the famous \href{http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data}{iris data set} because of its simplicity and low number of rows. The data consists of 150 rows and 5 features in each row: \emph{sepal length}, \emph{sepal width}, \emph{petal length}, \emph{petal width} and \emph{species}. I used the data to create a classifier that predicts the iris species from the other attributes.

  \section{Algorithm}
  I implemented the k-nearest-neighbors algorithm and leave-one-out cross-validation for it in Python. Pseudocode of the program is presented below.\\

  \begin{algorithm}[H]
   \KwData{Iris data}
   \KwResult{Misclassification rate}

   \emph{misclassifications} = 0\\
   \For{All items in data}{
      Choose \emph{test data}\\
      \emph{training data} $\leftarrow$ \{ All data $\backslash$ \emph{test data} \}\\
      Compute $k$ nearest neighbors of \emph{test data} in \emph{training data}\\
      Determine the majority class of the nearest neighbors\\
      \uIf{predicted wrong class}{
          accumulate \emph{misclassifications}\\

      }
   }
   \Return{misclassifications / number of predictions}

  \end{algorithm}

  \section{Results}
  \begin{table}[h]
  \centering
  \begin{tabular}{l|r}
  k & Misclassification rate \\\hline
  1 & 0.04\\
  2 & 0.05333...\\
  3 & 0.04\\
  5 & 0.0333...\\
  8 & 0.0333...\\
  13 & 0.0333...\\
  21 & 0.02\\
  34 & 0.0466...
  \end{tabular}

  \caption{\label{tab:widgets}Misclassification rates on different values of k}
  \end{table}
  \noindent
  It seems that choosing $k=21$ will provides the best accuracy with a misclassification rate of 0.02. I am quite pleased with this model.

  \section{Source code}
  Below is listed the source code of the program. For syntax highlighting, please visit my \href{https://github.com/jootimo/ml_repo}{GitHub page}. There are two Python files, $\href{https://github.com/jootimo/ml_repo/blob/master/iris_classification.py}{iris\_classification.py}$ handles opening and parsing the data file and $\href{https://github.com/jootimo/ml_repo/blob/master/knn.py}{knn.py}$ contains the actual k-nearest-neighbors implementation. 

  Function  $\emph{classification\_with\_cross\_validation()}$ in $\emph{knn.py}$ iteratively divides the data into a test row and training set, computes the distances between the test row and the rows in the training set, predicts the class for the test row and counts the misclassification rate of the predictions. \\\\


  \renewcommand{\sfdefault}{pcr}
  \fontfamily{pcr}\selectfont

  \noindent
  iris\_classification.py:\\
  \begin{lstlisting}
  DATA_FILENAME = "data/iris.data"

  #Classes as binary values
  IRIS_SETOSA =       0b001
  IRIS_VERSICOLOUR =  0b010
  IRIS_VIRGINICA =    0b100

  #Indices of feature names
  IX_SEPAL_LENGTH =   0
  IX_SEPAL_WIDTH =    1
  IX_PETAL_LENGTH =   2
  IX_PETAL_WIDTH =    3
  IX_IRIS_CLASS =     4

  NUM_FEATURES =      5

  # Open file with name filename and parse comma separated values a into 2-d list
  #
  # @param    filename
  def open_and_parse(filename):
      data = []

      with open(filename, "r") as filestream:

          for line in iter(filestream.readline, ''):
              current_line = line.split(',')

              row = [0] * NUM_FEATURES

              row[IX_SEPAL_LENGTH]  = float(current_line[0])
              row[IX_SEPAL_WIDTH]  = float(current_line[1])
              row[IX_PETAL_LENGTH]  = float(current_line[2])
              row[IX_PETAL_WIDTH]  = float(current_line[3])

              #Transform the class strings into numerical values
              current_class = current_line[4]
              if current_class == "Iris-setosa\n" or current_class == "Iris-setosa":
                  row[IX_IRIS_CLASS]  = IRIS_SETOSA

              if current_class == "Iris-versicolor\n" or current_class == "Iris-versicolor":
                  row[IX_IRIS_CLASS]  = IRIS_VERSICOLOUR

              if current_class == "Iris-virginica\n" or current_class == "Iris-virginica":
                  row[IX_IRIS_CLASS]  = IRIS_VIRGINICA


              data.append(row)

      return data

  #Open file and read data into memory
  data = open_and_parse(DATA_FILENAME)

  #Test the performance k-nearest-neighbors with different values of k
  ks_to_test = [1, 2, 3, 5, 8, 13, 21, 34]
  for k in ks_to_test:
      print("k = " + str(k))
      print("Misclassification rate = " 
      + str(knn.classification_with_cross_validation(data, IX_IRIS_CLASS, k)))

  \end{lstlisting}
  \vspace*{3cm}
  knn.py:\\

  \begin{lstlisting}

  # Calculate the Euclidean distance between data points a and b
  #
  # @param    a   list of attribute values
  # @param    b   list of attribute values
  # @return   distance between a and b or -1 if lists have different lenghts 
  def distance(a, b):
      len_a = len(a)
      len_b = len(b)

      if len_a != len_b:
          return(-1)
      else:
          dist = 0
          for i in range(0, len_a):
              dist += (a[i] - b[i]) ** 2

          return sqrt(dist)


  # Compute distances from each data point in training data to that in test data
  #
  # @param    training_data  list of numerical data rows, excluding the attribute to predict
  # @param    test_data      list of the data object to calculate distances to
  # @return   distance list
  def compute_distances(test_data, training_data):
      num_rows = len(training_data)

      distances = []

      for i in range(0, num_rows):
          dist = distance(test_data, training_data[i])
          distances.append(dist)

      return(distances)


  # Compute nearest neighbors of the given data object
  #
  # @param    test_data       data object to compute distances to
  # @param    training_data   data that the distances are computed to
  # @param    num_neighbors   value of k
  # @return   num_neighbors nearest neighbors of test_data in training_data
  def compute_nearest_neighbors(test_data, training_data, num_neighbors):
      #Compute distances to test_data
      distances = compute_distances(test_data, training_data)

      #Iterate over the distance list and create list of (row index, distance) pairs
      ixs_and_distances = []
      for i in range(0, len(distances)):
          ixs_and_distances.append([i, distances[i]])

      #Sort the list on distances and return first num_neighbors elements
      ixs_and_distances.sort(key = itemgetter(1))
      return(ixs_and_distances[0 : num_neighbors])


  # Get the majority class in list neighbors
  #
  # @param    neighbors   list of (index, distance) pairs
  # @param    classes     list of the correct classes in the training data
  # @return   value of the majority class 
  def majority_class(neighbors, classes):
      neighbor_classes = []
      for n in neighbors:
          neighbor_classes.append(classes[n[0]])

      return(max(set(neighbor_classes), key = neighbor_classes.count))


  # Predict the class of test data object from it's nearest neighbors in training data
  #
  # @param    test_data       the data object whose class is predicted
  # @param    training_data   list of data objects where neighbors are searched
  # @param    target_list     correct classes of objects in training data
  # @param    k               number of nearest neighbors to search
  #  
  # @return   value of the predicted class
  def predict_class(test_data, training_data, target_list, k):

      neighbors = compute_nearest_neighbors(test_data, training_data, k)
      predicted_class = majority_class(neighbors, target_list)

      return(predicted_class)


  # Perform the k-nearest-neighbors classification with leave-one-out
  # cross-validiation on data
  #
  # @param    data        list of the data objects to analyze
  # @param    ix_target   index of the field whose value will be predicted
  # @param    k           number of neighbors to search
  #
  # @return   misclassification rate of the classification
  def classification_with_cross_validation(data, ix_target, k):
      data_without_targets = [[0] * len(data[0])] * len(data)
      rows = len(data_without_targets)

      #Delete the target attribute from data
      classes = []
      for i in range(0, rows):
          classes.append(data[i][ix_target])
          data_without_targets[i] = data[i][0 : ix_target]

      #Perform predictions and count misclassifications
      misclassifications = 0
      for i in range(0, rows):

          #Split data into single test object and training data
          test_data = data_without_targets[i]
          training_data = data_without_targets[:i] + data_without_targets[i + 1 : rows]

          #Predict a class for the test data
          predicted_class = predict_class(test_data, training_data, classes, k)

          if(predicted_class != classes[i]):
              misclassifications += 1

      #Calculate and return the misclassification rate 
      misclassification_rate = misclassifications / rows
      return misclassification_rate
  \end{lstlisting}



  \end{document}